{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework #1\n",
    "\n",
    "Your name: Yeongjun park\n",
    "\n",
    "### Question 1\n",
    "You will be performing one iteration of the forward pass and backpropagation calculations for a small network using Python. Here we will focus on the calculations for one training example, though in reality your data sets will be much larger and require matrix computation. You will also calculate the associated loss.\n",
    "\n",
    "Let $X_1 = 2$ and $X_2 = -1$ be the feature inputs and initialize the weights to be as shown in the figure below. This is a neural network with a single hidden layer consisting of three nodes. The blue numbers within each node represent the values for the bias terms and the black numbers along the edges represent the weights. The hidden layer outputs a single node, from which your task is binary classification. The label for this particular training example outcome is $y = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"simple_nn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a single forward pass of the network. You do not need to implement the network in keras and should instead use numpy operations (either scalar or matrix). Please use the variable names and print statements provided in the code chunks to display results for the TAs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptual explanation:\n",
    "\n",
    "The forward pass typically refers to the calculation process - values of the output layers given the input data. It is easier to think of it as a traversal process that neurons go through from the first to the last layer. The bakward pass refers to the process of counting changes in weights using a gradient descent algorithm. Computation starts from the last layer and backward to the first layer. \n",
    "\n",
    "Loss function is one of the performance metrics that examines how well the neural network manages to reach its goal of generating outputs close to the desired values. A loss function returns a single number, and our goal is to minimize the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass using numpy operations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Xs = np.array([[2,-1,1]])\n",
    "hidden_w = np.matrix([[1,0.2,-0.6],[1.1,0,-0.3],[-1.8,-0.4,0.96]])\n",
    "Ws = np.matrix([[.5],[.1],[1.3]])\n",
    "Bs = 2\n",
    "hidden = np.matmul(Xs, hidden_w)\n",
    "output = hidden*Ws + Bs\n",
    "y_hat = 1/(1+np.exp(-out))\n",
    "prediction = np.round(y_hat)\n",
    "print('The values for the hidden layer are:', hidden)\n",
    "print('The value for the output layer is:', output)\n",
    "print('The predicted probability is:', y_hat)\n",
    "print('The prediction is:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the loss for the training example making sure to select the appropriate loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: [[0.1792517]]\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "\n",
    "y_i = 1\n",
    "loss_i = (-y_i)*np.log(y_hat)-(1-y_i)*np.log(1-y_hat)\n",
    "print('The loss is:',loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a single backward pass of the network. Again use numpy and report the values using the print statements provided. Please interpret these values. In other words, what are the values you just calculated used for? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradients of the loss wrt to the hidden weights are: [[ 0.14769407  0.         -0.00984627]]\n",
      "The gradient of the loss wrt to the hidden bias is: [[-0.16410453]]\n",
      "The gradients of the loss wrt to the input weights going to hidden node 1 are: [[-0.08205226 -0.16410453  0.08205226]]\n",
      "The gradients of the loss wrt to the input weights going to hidden node 2 are: [[-0.01641045 -0.03282091  0.01641045]]\n",
      "The gradients of the loss wrt to the input weights going to hidden node 3 are: [[-0.21333588 -0.42667177  0.21333588]]\n"
     ]
    }
   ],
   "source": [
    "#  Backward pass of the network using numpy operations\n",
    "\n",
    "dl_dy = (y_hat-y_i)/(y_hat-y_hat**2)\n",
    "dy_dout = y_hat*(1-y_hat)\n",
    "dout_dw_h = hidden\n",
    "dout_db_h = 1\n",
    "x_hid = np.array([[1,2,-1]])\n",
    "dout_dh = np.transpose(w_out)\n",
    "\n",
    "dl_dw_h = dl_dy*dy_dout*dout_dw_h\n",
    "dl_db_h = dl_dy*dy_dout*dout_db_h\n",
    "dl_dw_1 = dl_dy*dy_dout*dout_dh[0,0]*x_hid\n",
    "dl_dw_2 = dl_dy*dy_dout*dout_dh[0,1]*x_hid\n",
    "dl_dw_3 = dl_dy*dy_dout*dout_dh[0,2]*x_hid\n",
    "print('The gradients of the loss wrt to the hidden weights are:', dl_dw_h)\n",
    "print('The gradient of the loss wrt to the hidden bias is:', dl_db_h)\n",
    "print('The gradients of the loss wrt to the input weights going to hidden node 1 are:', dl_dw_1)\n",
    "print('The gradients of the loss wrt to the input weights going to hidden node 2 are:', dl_dw_2)\n",
    "print('The gradients of the loss wrt to the input weights going to hidden node 3 are:', dl_dw_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "In class we were considering classification problems where the goal was to predict a single discrete label of an input data point. Another common type of machine learning problem is \"regression\", which consists of predicting a continuous value instead of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a software project will take to complete, given its specifications.\n",
    "\n",
    "You will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
    "\n",
    "The dataset you will be using has another interesting difference from our previous examples: it has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, others between 0 and 100.\n",
    "\n",
    "The data consists 13 features. The 13 features in the input data are as follows:\n",
    "\n",
    "1. Per capita crime rate.\n",
    "2. Proportion of residential land zoned for lots over 25,000 square feet.\n",
    "3. Proportion of non-retail business acres per town.\n",
    "4. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5. Nitric oxides concentration (parts per 10 million).\n",
    "6. Average number of rooms per dwelling.\n",
    "7. Proportion of owner-occupied units built prior to 1940.\n",
    "8. Weighted distances to five Boston employment centres.\n",
    "9. Index of accessibility to radial highways.\n",
    "10. Full-value property-tax rate per $10,000.\n",
    "11. Pupil-teacher ratio by town.\n",
    "12. 1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town.\n",
    "13. % lower SES status of the population.\n",
    "\n",
    "The targets (outcomes, y) are the median values of owner-occupied homes, in thousands of dollars. The prices are typically between 10,000 and 50,000 dollars. If that sounds cheap, remember this was the mid-1970s, and these prices are not inflation-adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "# Tensorflow must be installed via Linux before installing Keras\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dimensions of the training set, i.e. its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the training set are: (404, 13)\n"
     ]
    }
   ],
   "source": [
    "# Training shape\n",
    "print('The dimensions of the training set are:', train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dimensions of the test set, i.e. its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the test set are: (102, 13)\n"
     ]
    }
   ],
   "source": [
    "# test shape\n",
    "print('The dimensions of the test set are:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation.\n",
    "\n",
    "Normalize the data. Be sure to normalize the test set with the training set mean and standard deviation.\n",
    "\n",
    "* Conceptual explanation:\n",
    "\n",
    "In many machine learning algorithms, objective functions may not work propery without normalization because the range of values of raw data varies widely. Data is normalized feather-wise for the given dataset since the aim is to investigate the relationship across data and having the ability to predict well with the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a fully connected neural network with 2 hidden layers and an output layer. Include 64 hidden units in each hidden layer and an appropriate number of units in the output layer. You are free to choose the activation functions. Use the `rmsprop` optimization function, and choose an appropriate loss function and model performance measure. Referring to the table shown in lectures 2 and 3 may help with these choices. Run the network for 50 epochs and use a batch_size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 448.2255 - mean_absolute_error: 18.9406\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 0s 104us/step - loss: 179.4268 - mean_absolute_error: 10.8643\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 0s 92us/step - loss: 58.3466 - mean_absolute_error: 5.6460\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 0s 87us/step - loss: 33.4349 - mean_absolute_error: 4.0641\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 23.5308 - mean_absolute_error: 3.3540\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 0s 84us/step - loss: 19.1907 - mean_absolute_error: 2.9280\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 16.8196 - mean_absolute_error: 2.8341\n",
      "Epoch 8/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 15.2639 - mean_absolute_error: 2.6749\n",
      "Epoch 9/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 13.6945 - mean_absolute_error: 2.5449\n",
      "Epoch 10/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 12.8112 - mean_absolute_error: 2.5260\n",
      "Epoch 11/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 11.9618 - mean_absolute_error: 2.4040\n",
      "Epoch 12/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 11.6883 - mean_absolute_error: 2.4002\n",
      "Epoch 13/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 11.0406 - mean_absolute_error: 2.3206\n",
      "Epoch 14/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 10.8674 - mean_absolute_error: 2.2970\n",
      "Epoch 15/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 10.4240 - mean_absolute_error: 2.2728\n",
      "Epoch 16/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 10.1772 - mean_absolute_error: 2.2295\n",
      "Epoch 17/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: 10.0048 - mean_absolute_error: 2.2174\n",
      "Epoch 18/50\n",
      "404/404 [==============================] - 0s 88us/step - loss: 9.5931 - mean_absolute_error: 2.1504\n",
      "Epoch 19/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 9.7306 - mean_absolute_error: 2.2022\n",
      "Epoch 20/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: 9.4103 - mean_absolute_error: 2.1420\n",
      "Epoch 21/50\n",
      "404/404 [==============================] - 0s 90us/step - loss: 9.3837 - mean_absolute_error: 2.1548\n",
      "Epoch 22/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 8.7554 - mean_absolute_error: 2.1072\n",
      "Epoch 23/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 9.0359 - mean_absolute_error: 2.1844\n",
      "Epoch 24/50\n",
      "404/404 [==============================] - 0s 81us/step - loss: 8.9178 - mean_absolute_error: 2.0881\n",
      "Epoch 25/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 8.5385 - mean_absolute_error: 2.0534\n",
      "Epoch 26/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 8.8667 - mean_absolute_error: 2.0872\n",
      "Epoch 27/50\n",
      "404/404 [==============================] - 0s 83us/step - loss: 8.6765 - mean_absolute_error: 2.0757\n",
      "Epoch 28/50\n",
      "404/404 [==============================] - 0s 93us/step - loss: 8.3984 - mean_absolute_error: 2.0234\n",
      "Epoch 29/50\n",
      "404/404 [==============================] - 0s 90us/step - loss: 8.2907 - mean_absolute_error: 2.0280\n",
      "Epoch 30/50\n",
      "404/404 [==============================] - 0s 92us/step - loss: 8.4255 - mean_absolute_error: 2.0613\n",
      "Epoch 31/50\n",
      "404/404 [==============================] - 0s 97us/step - loss: 8.0261 - mean_absolute_error: 2.0291\n",
      "Epoch 32/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 8.1661 - mean_absolute_error: 1.9846\n",
      "Epoch 33/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: 8.0992 - mean_absolute_error: 2.0160\n",
      "Epoch 34/50\n",
      "404/404 [==============================] - 0s 81us/step - loss: 7.7680 - mean_absolute_error: 1.9752\n",
      "Epoch 35/50\n",
      "404/404 [==============================] - 0s 82us/step - loss: 8.0079 - mean_absolute_error: 1.9811\n",
      "Epoch 36/50\n",
      "404/404 [==============================] - 0s 92us/step - loss: 7.7695 - mean_absolute_error: 1.9495\n",
      "Epoch 37/50\n",
      "404/404 [==============================] - 0s 88us/step - loss: 7.4337 - mean_absolute_error: 1.9550\n",
      "Epoch 38/50\n",
      "404/404 [==============================] - 0s 82us/step - loss: 7.7404 - mean_absolute_error: 1.9540\n",
      "Epoch 39/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 7.5542 - mean_absolute_error: 1.9319\n",
      "Epoch 40/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 7.6323 - mean_absolute_error: 1.9624\n",
      "Epoch 41/50\n",
      "404/404 [==============================] - 0s 83us/step - loss: 7.3919 - mean_absolute_error: 1.8892\n",
      "Epoch 42/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 7.3675 - mean_absolute_error: 1.9163\n",
      "Epoch 43/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 7.2570 - mean_absolute_error: 1.8761\n",
      "Epoch 44/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 7.5516 - mean_absolute_error: 1.9157\n",
      "Epoch 45/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 7.2515 - mean_absolute_error: 1.8855\n",
      "Epoch 46/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 7.0960 - mean_absolute_error: 1.8592\n",
      "Epoch 47/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 7.0736 - mean_absolute_error: 1.8419\n",
      "Epoch 48/50\n",
      "404/404 [==============================] - 0s 87us/step - loss: 7.0888 - mean_absolute_error: 1.8565\n",
      "Epoch 49/50\n",
      "404/404 [==============================] - 0s 83us/step - loss: 7.0359 - mean_absolute_error: 1.8619\n",
      "Epoch 50/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 6.9154 - mean_absolute_error: 1.8766\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network with 2 hidden layers - 64 hidden units in each layer\n",
    "\n",
    "network64 = models.Sequential()\n",
    "network64.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "network64.add(layers.Dense(64, activation = 'relu'))\n",
    "network64.add(layers.Dense(1))\n",
    "network64.compile(loss='mean_squared_error',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['mae'])\n",
    "\n",
    "history = network64.fit(train_data, train_targets, batch_size = 10, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the test set accuracy and compare it to the training set accuracy. **Interpret what this means in words, in terms of what you are trying to do with your network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 2ms/step\n",
      "test accuracy: 2.701396072612089\n",
      "404/404 [==============================] - 0s 19us/step\n",
      "train accuracy: 1.7768920553792822\n"
     ]
    }
   ],
   "source": [
    "# Test Loss versus Training Loss\n",
    "test_loss, test_acc = network64.evaluate(test_data, test_targets)\n",
    "print('test accuracy:', test_acc)\n",
    "train_loss, train_acc = network64.evaluate(train_data, train_targets)\n",
    "print('train accuracy:', train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptual explanation:\n",
    "\n",
    "`rmsprop` lies in the realm of adaptive learning rate methods. It derives the learning rate by an exponential decaying average of squared gradients. `Mean absolute error (MAE)` is used as the performance metric. MAE measures the average magnitude of the errors from a set of predictions. It is the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight. Given this, a smaller MAE indicates a better performance. `mean squared error (MSE)` is used to compute the loss where a smaller MSE is preferred. Initially, a `sigmoid` activation function is considered and implemented; however, a `relu` activation performs much better than a `sigmoid` activation in terms of MSE (smaller loss) and MAE (smaller error). Therefore, my network is implemented using a `relu` activation function. \n",
    "\n",
    "* Accuracy comparison: \n",
    "\n",
    "The training accuracy is approximately 1.78 where the test accuracy is approximately 2.70. It makes sense why the training phase performs better because a training phase of machine learning implementation works with data from the gold standard by pairing the input with expected output. Testing phase estimates how well the model does from a different set of data than the data used from the training phase. However, the two accuracy metrics do not differ much, and our model performs well in both training and testing datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the same network as above but with 16 hidden nodes in each hidden layer. **What is the test set accuracy and how does it compare to the first network you created? Which model do you think is better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 581.5797 - mean_absolute_error: 22.3596\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 0s 114us/step - loss: 533.5237 - mean_absolute_error: 21.4300\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: 478.3840 - mean_absolute_error: 20.2942\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 409.4852 - mean_absolute_error: 18.7947\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 0s 99us/step - loss: 329.2289 - mean_absolute_error: 16.7740\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: 242.9070 - mean_absolute_error: 14.1310\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 0s 92us/step - loss: 167.7737 - mean_absolute_error: 11.0639\n",
      "Epoch 8/50\n",
      "404/404 [==============================] - 0s 83us/step - loss: 114.7355 - mean_absolute_error: 8.6380\n",
      "Epoch 9/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 80.6101 - mean_absolute_error: 6.7585\n",
      "Epoch 10/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 57.7893 - mean_absolute_error: 5.4870\n",
      "Epoch 11/50\n",
      "404/404 [==============================] - ETA: 0s - loss: 21.4218 - mean_absolute_error: 4.06 - 0s 87us/step - loss: 42.6740 - mean_absolute_error: 4.5084\n",
      "Epoch 12/50\n",
      "404/404 [==============================] - 0s 97us/step - loss: 34.7227 - mean_absolute_error: 3.9561\n",
      "Epoch 13/50\n",
      "404/404 [==============================] - 0s 84us/step - loss: 31.1548 - mean_absolute_error: 3.7761\n",
      "Epoch 14/50\n",
      "404/404 [==============================] - 0s 84us/step - loss: 28.8137 - mean_absolute_error: 3.6459\n",
      "Epoch 15/50\n",
      "404/404 [==============================] - 0s 90us/step - loss: 26.9401 - mean_absolute_error: 3.5312\n",
      "Epoch 16/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 25.3752 - mean_absolute_error: 3.4453\n",
      "Epoch 17/50\n",
      "404/404 [==============================] - 0s 79us/step - loss: 23.9957 - mean_absolute_error: 3.3738\n",
      "Epoch 18/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 22.8420 - mean_absolute_error: 3.2794\n",
      "Epoch 19/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 21.5938 - mean_absolute_error: 3.2248\n",
      "Epoch 20/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 20.4968 - mean_absolute_error: 3.1701\n",
      "Epoch 21/50\n",
      "404/404 [==============================] - 0s 100us/step - loss: 19.5076 - mean_absolute_error: 3.0907\n",
      "Epoch 22/50\n",
      "404/404 [==============================] - 0s 93us/step - loss: 18.6034 - mean_absolute_error: 3.0172\n",
      "Epoch 23/50\n",
      "404/404 [==============================] - 0s 86us/step - loss: 17.7603 - mean_absolute_error: 2.9522\n",
      "Epoch 24/50\n",
      "404/404 [==============================] - 0s 88us/step - loss: 17.0415 - mean_absolute_error: 2.8949\n",
      "Epoch 25/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 16.3061 - mean_absolute_error: 2.8560\n",
      "Epoch 26/50\n",
      "404/404 [==============================] - 0s 82us/step - loss: 15.7591 - mean_absolute_error: 2.8013\n",
      "Epoch 27/50\n",
      "404/404 [==============================] - 0s 87us/step - loss: 15.1911 - mean_absolute_error: 2.7599\n",
      "Epoch 28/50\n",
      "404/404 [==============================] - 0s 82us/step - loss: 14.7613 - mean_absolute_error: 2.7511\n",
      "Epoch 29/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 14.2570 - mean_absolute_error: 2.6937\n",
      "Epoch 30/50\n",
      "404/404 [==============================] - 0s 80us/step - loss: 13.7974 - mean_absolute_error: 2.6546\n",
      "Epoch 31/50\n",
      "404/404 [==============================] - 0s 94us/step - loss: 13.4051 - mean_absolute_error: 2.6172\n",
      "Epoch 32/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 13.0619 - mean_absolute_error: 2.5957\n",
      "Epoch 33/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 12.7630 - mean_absolute_error: 2.5629\n",
      "Epoch 34/50\n",
      "404/404 [==============================] - 0s 87us/step - loss: 12.4265 - mean_absolute_error: 2.5099\n",
      "Epoch 35/50\n",
      "404/404 [==============================] - 0s 90us/step - loss: 12.1437 - mean_absolute_error: 2.5037\n",
      "Epoch 36/50\n",
      "404/404 [==============================] - 0s 90us/step - loss: 11.9909 - mean_absolute_error: 2.5029\n",
      "Epoch 37/50\n",
      "404/404 [==============================] - 0s 81us/step - loss: 11.6981 - mean_absolute_error: 2.4684\n",
      "Epoch 38/50\n",
      "404/404 [==============================] - 0s 76us/step - loss: 11.5222 - mean_absolute_error: 2.4511\n",
      "Epoch 39/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 11.3091 - mean_absolute_error: 2.4118\n",
      "Epoch 40/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 11.1608 - mean_absolute_error: 2.3940\n",
      "Epoch 41/50\n",
      "404/404 [==============================] - 0s 97us/step - loss: 10.9646 - mean_absolute_error: 2.3710\n",
      "Epoch 42/50\n",
      "404/404 [==============================] - 0s 85us/step - loss: 10.8379 - mean_absolute_error: 2.3760\n",
      "Epoch 43/50\n",
      "404/404 [==============================] - 0s 89us/step - loss: 10.6613 - mean_absolute_error: 2.3528\n",
      "Epoch 44/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: 10.5791 - mean_absolute_error: 2.3422\n",
      "Epoch 45/50\n",
      "404/404 [==============================] - 0s 78us/step - loss: 10.4008 - mean_absolute_error: 2.3155\n",
      "Epoch 46/50\n",
      "404/404 [==============================] - 0s 75us/step - loss: 10.3926 - mean_absolute_error: 2.3162\n",
      "Epoch 47/50\n",
      "404/404 [==============================] - 0s 74us/step - loss: 10.2255 - mean_absolute_error: 2.3076\n",
      "Epoch 48/50\n",
      "404/404 [==============================] - 0s 73us/step - loss: 10.2199 - mean_absolute_error: 2.2954\n",
      "Epoch 49/50\n",
      "404/404 [==============================] - 0s 74us/step - loss: 10.0339 - mean_absolute_error: 2.3001\n",
      "Epoch 50/50\n",
      "404/404 [==============================] - 0s 77us/step - loss: 9.9143 - mean_absolute_error: 2.2757\n",
      "102/102 [==============================] - 0s 2ms/step\n",
      "404/404 [==============================] - 0s 15us/step\n",
      "test accuracy: 3.108356475830078\n",
      "train accuracy: 2.2368764948136737\n"
     ]
    }
   ],
   "source": [
    "# Model with 16 hidden nodes in each hidden layer\n",
    "\n",
    "network16 = models.Sequential()\n",
    "network16.add(layers.Dense(16, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "network16.add(layers.Dense(16, activation = 'relu'))\n",
    "network16.add(layers.Dense(1))\n",
    "network16.compile(loss='mean_squared_error',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['mae'])\n",
    "\n",
    "history1 = network16.fit(train_data, train_targets, batch_size = 10, epochs=50)\n",
    "\n",
    "test_loss1, test_acc1 = network16.evaluate(test_data, test_targets)\n",
    "train_loss1, train_acc1 = network16.evaluate(train_data, train_targets)\n",
    "print('test accuracy:', test_acc1)\n",
    "print('train accuracy:', train_acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xb29aa13c8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_loss1 = history1.history['loss']\n",
    "\n",
    "epochs = list(range(1, len(train_loss) + 1))\n",
    "plt.plot(epochs, train_loss, 'b', label = 'Larget network (64) Loss', color = 'red')\n",
    "plt.plot(epochs, train_loss1, 'b', label = \"Smaller network (16) Loss\")\n",
    "plt.title('Loss comparison two different networks')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model comparison: \n",
    "\n",
    "When there are 16 nodes in each hidden layer, both the training and testing datasets perform worse compared to the previous example where 64 nodes are implemented for each hidden layer. The training accuracy is approximately 2.24 where the testing accuracy is approximately 3.11. Same as the previous example, the training dataset performs better than the testing dataset. Therefore, a previous implementation with the 64 hidden units in each layer better predicts Boston median housing prices duing the 1970's. \n",
    "\n",
    "There are diffferent expectations and standards when it comes to implementing an ideal number of neurons in each hidden layer. There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layer' according to the author of `Introduction to Neural Networks in Java`, Jeff Heaton. Given the size of the input and the output layer, it is thus unsurprising that the network using larger size of the hidden layers performs  better. We can also see from the plot above that the larger network (64 hidden units in each layer) has smaller loss through different epochs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
